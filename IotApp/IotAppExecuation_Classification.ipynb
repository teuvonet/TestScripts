{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae019fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import paho.mqtt.client as client\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae80e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "execure_url = 'http://0.0.0.0:5000/execute'\n",
    "fetch_url = 'http://0.0.0.0:5000/fetch'\n",
    "iot_app_path = \"/home/asim/takshshila/IOT/IoTApp/app.py\"\n",
    "script_directory = \"/home/asim/takshshila/IOT/Datasets/setup-stuff/gateway_and_dataset\"\n",
    "data_directory = \"/home/asim/takshshila/IOT/Datasets/DATASETS_CLASSIFICATION/BINARY_PROBLEMS\"\n",
    "result_dir = \"/home/asim/takshshila/IOT/Datasets/setup-stuff/gateway_and_dataset/Results_Classification\"\n",
    "algorithm = \"classification\"\n",
    "\n",
    "net_sizes = [4,5]\n",
    "#net_sizes = [3, 4, 5]\n",
    "#net_sizes = [3,4,5,6,7]\n",
    "# net_sizes = [6,7]\n",
    "number_of_shuffles = 4\n",
    "convergence_threshold = 0.05\n",
    "num_chunks=5 # size of chunks 50*\n",
    "data_passes=400 #number of chunks\n",
    "chunk_passes= 30  # epochs\n",
    "learning_rates =[0.1, 0.05, 0.075,0.008]#[0.1, 0.075, 0.009] #[0.05, 0.002,0.0005]#[0.1, 0.075, 0.008]#[0.01, 0.075 ,0.002, 0.0005]#[0.002, 0.005, 0.0008] #[0.1, 0.05, 0.075]\n",
    "partition_size = 100\n",
    "\n",
    "sessionID= \"06f0483a-987c-11ed-97d7-ebb1648f0903\" #\"5ec46db2-92b3-11ed-aaca-51e78ce51e2f\"#\"30929878-9043-11ed-a725-8326e5fa41d3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48338cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prereqObj={\n",
    "    \"phase\": \"prereq\",\n",
    "    \"mode\":\"mqtt\",\n",
    "    \"algorithm\": algorithm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603a9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase0Obj={\n",
    "    \"phase\": \"phase0\",\n",
    "    \"num_chunks\": num_chunks,\n",
    "    \"mode\":\"mqtt\",\n",
    "    \"algorithm\": algorithm,\n",
    "    \"session_id\": sessionID\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c669e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1Obj={\n",
    "    \"phase\": \"phase1\",\n",
    "    \"data_passes\": data_passes,\n",
    "    \"net_sizes\": net_sizes,\n",
    "    \"chunk_passes\": chunk_passes,\n",
    "    \"num_chunks\": num_chunks,\n",
    "    \"num_classes\":2,\n",
    "    \"learning_rates\": learning_rates,\n",
    "    \"number_of_shuffles\":number_of_shuffles,\n",
    "    \"num_feature_spaces\": 1,\n",
    "    \"neigh_rate\": 0.8,\n",
    "    \"train_test_split\": 1,\n",
    "    \"phase3_passes\": 1,\n",
    "    \"top_ranks\": 2,\n",
    "    \"partition_size\": partition_size,\n",
    "    \"neuron_init_criteria\":\"farthest_point\",\n",
    "    \"algorithm\": algorithm,\n",
    "    \"mode\":\"mqtt\",\n",
    "    \"session_id\": sessionID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c98ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2Obj = {\n",
    "    \"phase\": \"phase2\",\n",
    "    \"data_passes\": data_passes,\n",
    "    \"net_sizes\": net_sizes,\n",
    "    \"chunk_passes\": chunk_passes,\n",
    "    \"num_chunks\": num_chunks,\n",
    "    \"learning_rates\": learning_rates,\n",
    "    \"convergence_threshold\":convergence_threshold,\n",
    "    \"num_feature_spaces\": 1,\n",
    "    \"neigh_rate\": 0.8,\n",
    "    \"train_test_split\": 1,\n",
    "    \"phase3_passes\": 1,\n",
    "    \"top_ranks\": 100,\n",
    "    \"partition_size\": 100,\n",
    "    \"neuron_init_criteria\":\"farthest_point\",\n",
    "    \"mode\":\"mqtt\",\n",
    "    \"algorithm\": algorithm,\n",
    "    \"session_id\": sessionID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e843c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3Obj = {\n",
    "    \"phase\": \"phase3\",\n",
    "    \"data_passes\": data_passes,\n",
    "    \"net_sizes\": net_sizes,\n",
    "    \"chunk_passes\": chunk_passes,\n",
    "    \"learning_rates\": learning_rates,\n",
    "    \"num_chunks\": num_chunks,\n",
    "    \"num_feature_spaces\": 1,\n",
    "    \"neigh_rate\": 0.8,\n",
    "    \"train_test_split\": 1,\n",
    "    \"phase3_passes\": 1,\n",
    "    \"top_ranks\": 2,\n",
    "    \"partition_size\": 100,\n",
    "    \"mode\":\"mqtt\",\n",
    "    \"algorithm\": algorithm,\n",
    "    \"session_id\": sessionID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a283a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(algorithm, body):\n",
    "    r = requests.post(execure_url, json=body)\n",
    "    prereq_res = r.json()\n",
    "    return prereq_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886e5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_results(algorithm, phase, session_id):\n",
    "    body = {\"phase\": phase, \"algorithm\": algorithm,\"session_id\": session_id}\n",
    "    r = requests.post(fetch_url, json=body)\n",
    "    prereq_res = r.json()\n",
    "    return prereq_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98da87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(data_dir, dataset, seed, short, sep=','):\n",
    "    proc, gateway_proc, details = None, None, {}\n",
    "    try:\n",
    "        gateway_proc = subprocess.Popen(['python', os.path.join(script_directory, 'gateway_simulation.py')])\n",
    "        proc = subprocess.Popen(['python', os.path.join(script_directory, 'sonar_stream_V2.py'), dataset, str(seed), short, sep])\n",
    "    #       proc = subprocess.Popen(['python', r\"/home/asim/takshshila/IOT/TestScripts/sonar_stream_V2.py\", dataset, str(seed), short, sep, split])\n",
    "        backend_proc = subprocess.Popen(['python3', iot_app_path])\n",
    "\n",
    "        time.sleep(10)\n",
    "        print(\"Dataset ### \", dataset)\n",
    "        print(\"Prereq phase:::: \")\n",
    "        prereqResult = run_phase(algorithm, prereqObj)\n",
    "\n",
    "        print(\"Prereq result:::: \", prereqResult)\n",
    "        sessionID = prereqResult['body']['session-id']\n",
    "        \n",
    "        numClasses=prereqResult['body']['num_classes']\n",
    "        time.sleep(10)\n",
    "        \n",
    "    \n",
    "        print(\"Phase 0:::: \")\n",
    "        phase0Obj['session_id'] = sessionID\n",
    "        phase0Result = run_phase(algorithm, phase0Obj)\n",
    "        time.sleep(10)\n",
    "\n",
    "        print(\"Phase 1:::: \")\n",
    "        phase1Obj['session_id'] = sessionID\n",
    "        phase1Obj['num_classes'] = numClasses\n",
    "        phase1Result =run_phase(algorithm, phase1Obj)\n",
    "        time.sleep(10)\n",
    "\n",
    "        print(\"Phase 2:::: \")\n",
    "        phase2Obj['session_id'] = sessionID\n",
    "        phase2Obj['num_classes'] = numClasses\n",
    "        phase2Result =run_phase(algorithm, phase2Obj)\n",
    "        time.sleep(10)\n",
    "\n",
    "       \n",
    "        print(\"Fetching Results :::: \")\n",
    "        phase2Result=fetch_results(algorithm, \"phase2\", sessionID)\n",
    "        if phase2Result['success']:\n",
    "            details ={\n",
    "                'dataset': dataset,\n",
    "                'seed': seed,\n",
    "                'session_Id':sessionID,\n",
    "                'precision': phase2Result['body']['precision'],\n",
    "                'recalls': phase2Result['body']['recalls'],\n",
    "                'f1scores': phase2Result['body']['f1scores'],\n",
    "                'accuracies':phase2Result['body']['accuracies'],\n",
    "                'total_kernel_time':phase2Result['body']['kernel_time'],\n",
    "                \n",
    "                'distance_base_map':phase2Result['body']['distance_base_map'],\n",
    "                'radius_map':phase2Result['body']['radius_map'],\n",
    "                'active_centers_dominant_class_count':phase2Result['body']['active_centers_dominant_class_count'],\n",
    "                \n",
    "                'active_centers_per_class':phase2Result['body']['active_centers_per_class'],\n",
    "                'active_centers_total_count':phase2Result['body']['active_centers_total_count'],\n",
    "                'active_centers_dominant_class':phase2Result['body']['active_centers_dominant_class'],\n",
    "                'neuron_centers':phase2Result['body']['neuron_centers'],\n",
    "                'feature_order':phase2Result['body']['feature_order'],\n",
    "                \n",
    "            }\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        backend_proc.kill()\n",
    "        proc.kill()\n",
    "        gateway_proc.kill()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        if backend_proc:\n",
    "            backend_proc.kill()\n",
    "        if proc:\n",
    "            proc.kill()\n",
    "        if gateway_proc:\n",
    "            gateway_proc.kill()\n",
    "\n",
    "    return details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonar connected\n",
      " * Serving Flask app \"app\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ###  Adult\n",
      "Prereq phase:::: \n",
      "connecting to broker\n",
      "Subscribing to all topics\n",
      "adding topic to latest_elements \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Mar/2023 08:44:21] \"\u001b[37mPOST /execute HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prereq result::::  {'success': True, 'body': {'session-id': 'bd19fce2-c5a3-11ed-acbb-679ecd411b4c', 'num_classes': 2}}\n",
      "Phase 0:::: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Mar/2023 08:44:41] \"\u001b[37mPOST /execute HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1:::: \n",
      "API called: /execute\n",
      "\n",
      "\n",
      "Class: PhaseInjector\n",
      "Operation: Phase execution injection\n",
      "Phase map: {'phase': 'prereq', 'mode': 'mqtt', 'algorithm': 'classification'}\n",
      "\n",
      "\n",
      "Class: PrereqPhase\n",
      "Operation: Prereq phase construction\n",
      "Phase configs: {'phase': 'prereq', 'mode': 'mqtt', 'algorithm': 'classification'}\n",
      "\n",
      "\n",
      "Class: PrereqPhase\n",
      "Operation: Execution\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'mode': 'mqtt', 'client_name': 'ZCU1', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 2}\n",
      "\n",
      "\n",
      "Class: MQTTMeta\n",
      "Operation: MQTT client construction\n",
      "Client topic: from/zcu\n",
      "Subscription Topic: from/gateway\n",
      "\n",
      "\n",
      "Connected to MQTT Broker! send_train_data\n",
      "API called: /execute\n",
      "\n",
      "\n",
      "Class: PhaseInjector\n",
      "Operation: Phase execution injection\n",
      "Phase map: {'phase': 'phase0', 'num_chunks': 5, 'mode': 'mqtt', 'algorithm': 'classification', 'session_id': 'bd19fce2-c5a3-11ed-acbb-679ecd411b4c'}\n",
      "\n",
      "\n",
      "Class: Phase0\n",
      "Operation: Phase 0 construction\n",
      "Phase configs: {'phase': 'phase0', 'num_chunks': 5, 'mode': 'mqtt', 'algorithm': 'classification', 'session_id': 'bd19fce2-c5a3-11ed-acbb-679ecd411b4c'}\n",
      "\n",
      "\n",
      "Class: Phase0\n",
      "Operation: Execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'build_normalizer', 'algorithm': 'classification', 'feature_normalizer': None, 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Building chunk normalizer\n",
      "\n",
      "\n",
      "API called: /execute\n",
      "\n",
      "\n",
      "Class: PhaseInjector\n",
      "Operation: Phase execution injection\n",
      "Phase map: {'phase': 'phase1', 'data_passes': 400, 'net_sizes': [4, 5], 'chunk_passes': 30, 'num_chunks': 5, 'num_classes': 2, 'learning_rates': [0.1, 0.05, 0.075, 0.008], 'number_of_shuffles': 4, 'num_feature_spaces': 1, 'neigh_rate': 0.8, 'train_test_split': 1, 'phase3_passes': 1, 'top_ranks': 2, 'partition_size': 100, 'neuron_init_criteria': 'farthest_point', 'algorithm': 'classification', 'mode': 'mqtt', 'session_id': 'bd19fce2-c5a3-11ed-acbb-679ecd411b4c'}\n",
      "\n",
      "\n",
      "Class: Phase1\n",
      "Operation: Phase1 construction\n",
      "Phase configs: {'phase': 'phase1', 'data_passes': 400, 'net_sizes': [4, 5], 'chunk_passes': 30, 'num_chunks': 5, 'num_classes': 2, 'learning_rates': [0.1, 0.05, 0.075, 0.008], 'number_of_shuffles': 4, 'num_feature_spaces': 1, 'neigh_rate': 0.8, 'train_test_split': 1, 'phase3_passes': 1, 'top_ranks': 2, 'partition_size': 100, 'neuron_init_criteria': 'farthest_point', 'algorithm': 'classification', 'mode': 'mqtt', 'session_id': 'bd19fce2-c5a3-11ed-acbb-679ecd411b4c'}\n",
      "\n",
      "\n",
      "Class: Phase1\n",
      "Operation: Execution\n",
      "\n",
      "\n",
      "Class: Phase1\n",
      "Operation: Classification Execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: KernelInjector\n",
      "Operation: Kernel injection\n",
      "Mode: neuron_training\n",
      "\n",
      "\n",
      "Class: NeuronTraining\n",
      "Operation: Neuron training wrapper object construction\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel object construction\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: NeuronCenter\n",
      "Operation: Configuring and executing neuron centers init\n",
      "\n",
      "\n",
      "Class: NeuronCenterInit\n",
      "Operation: Neuron center init object construction\n",
      "Init configs: {'operation': 'init_neuron_centers', 'num_chunks': 1, 'init_criteria': 'farthest_point', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'net_sizes': array([4, 5], dtype=int32), 'num_features': 102, 'features_per_feature_spaces': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102], dtype=int32), 'learning_rates': array([0.1  , 0.05 , 0.075, 0.008], dtype=float32), 'features_all_feature_spaces': 5253, 'algorithm': 'classification', 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'num_chunks': 1, 'init_criteria': 'farthest_point', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'net_sizes': array([4, 5], dtype=int32), 'num_features': 102, 'features_per_feature_spaces': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102], dtype=int32), 'learning_rates': array([0.1  , 0.05 , 0.075, 0.008], dtype=float32), 'features_all_feature_spaces': 5253, 'algorithm': 'classification', 'target_to_id': {' <=50K': 0, ' >50K': 1}, 'normalizer_task': 'normalize'}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronCenterInit\n",
      "Operation: Neuron center init using farthest point\n",
      "\n",
      "\n",
      "Class: NeuronTraining\n",
      "Operation: Neuron training wrapper operation\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: Data normalization object construction\n",
      "\n",
      "\n",
      "Operation: Data stream injection\n",
      "Mode: mqtt\n",
      "\n",
      "\n",
      "Class: MQTT\n",
      "Operation: Constructing MQTT client and fetching data\n",
      "Fetch configs: {'operation': 'fetch_data', 'normalizer_task': 'normalize', 'algorithm': 'classification', 'feature_normalizer': MinMaxScaler(), 'target_normalizer': None, 'mode': 'mqtt', 'client_name': 'ZCU', 'client_topic': 'from/zcu', 'subscription': 'from/gateway', 'message': 'send_train_data', 'num_chunks': 5, 'target_to_id': {' <=50K': 0, ' >50K': 1}}\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n",
      "Operation: normalizing chunk\n",
      "\n",
      "\n",
      "Class: NeuronTrainingMeta\n",
      "Operation: Neuron training kernel execution\n",
      "\n",
      "\n",
      "Class: Mediator\n",
      "Operation: Configuring and executing Mediator\n",
      "\n",
      "\n",
      "Class: DataNormalization\n",
      "Operation: Configuring and executing chunk normalization\n",
      "\n",
      "\n",
      "Class: DataNormalizationMeta\n"
     ]
    }
   ],
   "source": [
    "#dataset_dir =r\"/home/asim/takshshila/Datasets/DATASETS_CLASSIFICATION/BINARY_PROBLEMS\"\n",
    "\n",
    "df_column_order = ['dataset','seed','session_Id','feature_order','total_kernel_time','precision','recalls','f1scores','accuracies','distance_base_map','radius_map','active_centers_per_class','active_centers_total_count','active_centers_dominant_class','neuron_centers']\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\"Adult\", \"Breast Cancer\", \"Credit Screening\", \"Ionosphere\", \"Liver Disorder\", \"Pima Indian\", \"Sonar\"]\n",
    "shorts = [\"AD\", \"BC\", \"CR\", \"IO\", \"LD\", \"PI\", \"SN\"] \n",
    "header = [False, False, False, False, False, False, False]\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "# datasets = [\"Adult\"]\n",
    "# shorts = [\"AD\"] \n",
    "# header = [False]\n",
    "# seeds = [1]\n",
    "\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    df = pd.DataFrame()\n",
    "    for seed in tqdm(seeds):\n",
    "        dataset = datasets[i]\n",
    "        short = shorts[i]\n",
    "        details = process_results(data_directory, dataset, seed, short)\n",
    "        df = df.append(details, ignore_index=True)\n",
    "        \n",
    "        filepath = Path(result_dir, dataset, \"classification_{}_{}_{}_{}_{}_{}_{}_{}.csv\".format(dataset, data_passes,chunk_passes,netsizeString, split, num_chunks,convergence_threshold, str(learning_rates)))\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df[df_column_order].to_csv(filepath, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946a810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761f610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76d801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba621c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f44836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2757e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c28bd77",
   "metadata": {},
   "source": [
    "## Test Running only phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(data_dir, dataset, seed, sessionID, split):\n",
    "    proc, gateway_proc, details = None, None, {}\n",
    "    try:\n",
    "        gateway_proc = subprocess.Popen(['python', os.path.join(data_dir, 'gateway_simulation.py')])\n",
    "        proc = subprocess.Popen(['python', os.path.join(data_dir, 'ECG_Stream_V2.py'), dataset, str(seed),split])\n",
    "\n",
    "        \n",
    "#         backend_proc = subprocess.Popen(['python3', iot_app_path])\n",
    "        \n",
    "        time.sleep(10)\n",
    "        \n",
    "        print(\"Prereq phase:::: \")\n",
    "        prereqResult = run_phase(algorithm, prereqObj)\n",
    "        print(\"Prereq result:::: \", prereqResult)\n",
    "        sessionID = prereqResult['body']['session-id']\n",
    "        time.sleep(10)\n",
    "        \n",
    "        print(\"Phase 0:::: \")\n",
    "        phase0Obj['session_id'] = sessionID\n",
    "        phase0Result = run_phase(algorithm, phase0Obj)\n",
    "        time.sleep(10)\n",
    "        \n",
    "        print(\"Phase 1:::: \")\n",
    "        phase1Obj['session_id'] = sessionID\n",
    "        phase1Result =run_phase(algorithm, phase1Obj)\n",
    "        time.sleep(10)\n",
    "        \n",
    "        print(\"Phase 2:::: \")\n",
    "        phase2Obj['session_id'] = sessionID\n",
    "        phase2Result =run_phase(algorithm, phase2Obj)\n",
    "        time.sleep(10)\n",
    "        \n",
    "        print(\"Phase 3:::: \")\n",
    "        phase3Obj['session_id'] = sessionID\n",
    "        phase3Result = run_phase(algorithm, phase3Obj)\n",
    "        time.sleep(10)\n",
    "        \n",
    "    \n",
    "        print(\"Fetching Results :::: \", sessionID)\n",
    "        phase3Result=fetch_results(algorithm, \"phase3\", sessionID)\n",
    "        if phase3Result['success']:\n",
    "            details ={\n",
    "                'dataset': dataset,\n",
    "                'seed': seed,\n",
    "                'session_Id':sessionID,\n",
    "                'train_rmse': phase3Result['body']['train_error'],\n",
    "                'valid_rmse': phase3Result['body']['validation_error'],\n",
    "                'test_rmse': phase3Result['body']['test_error'],\n",
    "                'best_score':phase3Result['body']['best_score'],\n",
    "                'scores':phase3Result['body']['scores'],\n",
    "                'val_rmses':phase3Result['body']['val_rmses'],\n",
    "                'train_rmses':phase3Result['body']['train_rmses'],\n",
    "                'best_val_lr':phase3Result['body']['best_val_lr'],\n",
    "                'best_fs_num_features':phase3Result['body']['best_fs_num_features'],\n",
    "                'best_model_active_linear_weights':phase3Result['body']['best_model_active_linear_weights'],\n",
    "                'best_model_active_gaussian_weights':phase3Result['body']['best_model_active_gaussian_weights'],\n",
    "                'bound_hitting_gaussian_weights':phase3Result['body']['bound_hitting_gaussian_weights'],\n",
    "                'neuron_centers':phase3Result['body']['neuron_centers'],\n",
    "                'neuron_activity':phase3Result['body']['neuron_activity'],\n",
    "                'radius_map':phase3Result['body']['radius_map'],\n",
    "                'targets':phase3Result['body']['targets'],\n",
    "                'predictions':phase3Result['body']['predictions'],\n",
    "                'total_kernel_time':phase3Result['body']['total_kernel_time'],\n",
    "                'total_host_time':phase3Result['body']['total_host_time'],\n",
    "                'phase1_kernel_time':phase3Result['body']['phase1_kernel_time'],\n",
    "                'phase1_host_time':phase3Result['body']['phase1_host_time'],\n",
    "                'phase2_kernel_time':phase3Result['body']['phase2_kernel_time'],\n",
    "                'phase2_host_time':phase3Result['body']['phase2_host_time'],\n",
    "                'phase3_kernel_time':phase3Result['body']['phase3_kernel_time'],\n",
    "                'phase3_host_time':phase3Result['body']['phase3_host_time'],\n",
    "                'best_features':phase3Result['body']['best_feature_list'],\n",
    "                'best_feature_names':phase3Result['body']['best_features_names'],\n",
    "            }\n",
    "            \n",
    "        time.sleep(20)\n",
    "        \n",
    "#         backend_proc.kill()\n",
    "        proc.kill()\n",
    "        gateway_proc.kill()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "#         if backend_proc:\n",
    "#             backend_proc.kill()\n",
    "        if proc:\n",
    "            proc.kill()\n",
    "        if gateway_proc:\n",
    "            gateway_proc.kill()\n",
    "            \n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_features','best_feature_names','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "\n",
    "datasets = [\"Facebook_data\"] #,\"dengue_features\"\n",
    "# sessionIDs = [\"8233474a-9bf6-11ed-bedf-c31d2beceb91\",\"78cfaabc-9bf7-11ed-bedf-c31d2beceb91\",\"71599a3a-9bf8-11ed-bedf-c31d2beceb91\",\"6b3a526a-9bf9-11ed-bedf-c31d2beceb91\",\n",
    "#               \"62183e12-9bfa-11ed-bedf-c31d2beceb91\",\"6745e76c-9bfb-11ed-bedf-c31d2beceb91\",\"676b4bdc-9bfc-11ed-bedf-c31d2beceb91\",\"5f363ae8-9bfd-11ed-bedf-c31d2beceb91\",\n",
    "#               \"5bcc2eca-9bfe-11ed-bedf-c31d2beceb91\",\"5c3392bc-9bff-11ed-bedf-c31d2beceb91\"]\n",
    "\n",
    "sessionIDs = [\"62183e12-9bfa-11ed-bedf-c31d2beceb91\"]\n",
    "df = pd.DataFrame()\n",
    "\n",
    "seeds = [1, 50, 100, 150,200, 250, 300, 350, 400, 450]\n",
    "# seeds = [200]\n",
    "netsizeString = \"_\".join([str(val) for val in net_sizes])\n",
    "split = \"0.9\"\n",
    "for index, dataset in enumerate(datasets):\n",
    "    for seed, session in tqdm(zip(seeds, sessionIDs)):\n",
    "        result = process_results(data_directory, dataset,seed, session, split)\n",
    "        df = df.append(result, ignore_index=True)\n",
    "        filepath = Path(result_dir, dataset, \"Test_{}_{}_{}_{}_{}_.csv\".format(dataset, data_passes,chunk_passes,netsizeString, split))\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df[df_column_order].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf0483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3063b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3Result=fetch_results(algorithm, \"phase3\", \"sessionId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b4c5b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storingPhase3(sessionId, dataset, seed):\n",
    "    phase3Result=fetch_results(algorithm, \"phase3\", sessionId)\n",
    "    if phase3Result['success']:        \n",
    "        details = {\n",
    "            'dataset': dataset,\n",
    "            'seed': seed,\n",
    "            'session':sessionId,\n",
    "            'train_rmse': phase3Result['body']['train_error'],\n",
    "            'valid_rmse': phase3Result['body']['validation_error'],\n",
    "            'test_rmse': phase3Result['body']['test_error'],\n",
    "            'best_score':phase3Result['body']['best_score'],\n",
    "            'scores':phase3Result['body']['scores'],\n",
    "            'val_rmses':phase3Result['body']['val_rmses'],\n",
    "            'train_rmses':phase3Result['body']['train_rmses'],\n",
    "            'best_val_lr':phase3Result['body']['best_val_lr'],\n",
    "            'best_fs_num_features':phase3Result['body']['best_fs_num_features'],\n",
    "            'best_model_active_linear_weights':phase3Result['body']['best_model_active_linear_weights'],\n",
    "            'best_model_active_gaussian_weights':phase3Result['body']['best_model_active_gaussian_weights'],\n",
    "            'bound_hitting_gaussian_weights':phase3Result['body']['bound_hitting_gaussian_weights'],\n",
    "            'neuron_centers':phase3Result['body']['neuron_centers'],\n",
    "            'neuron_activity':phase3Result['body']['neuron_activity'],\n",
    "            'radius_map':phase3Result['body']['radius_map'],\n",
    "            'targets':phase3Result['body']['targets'],\n",
    "            'predictions':phase3Result['body']['predictions'],\n",
    "            'total_kernel_time':phase3Result['body']['kernel_time'],\n",
    "            'total_host_time':phase3Result['body']['total_host_time'],\n",
    "            'phase1_kernel_time':phase3Result['body']['phase1_kernel_time'],\n",
    "            'phase1_host_time':phase3Result['body']['phase1_host_time'],\n",
    "            'phase2_kernel_time':phase3Result['body']['phase2_kernel_time'],\n",
    "            'phase2_host_time':phase3Result['body']['phase2_host_time'],\n",
    "            'phase3_kernel_time':phase3Result['body']['phase3_kernel_time'],\n",
    "            'phase3_host_time':phase3Result['body']['phase3_host_time'],\n",
    "            'best_feature_order':phase3Result['body']['best_feature_order'],\n",
    "            'best_feature_list':phase3Result['body']['best_feature_list'],\n",
    "\n",
    "            }\n",
    "        \n",
    "        return details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_feature_order','best_feature_list','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "\n",
    "dataset = \"Insurance\"\n",
    "seeds = [50, 100]\n",
    "sessionIds = [\"140c95c6-9789-11ed-893b-4f1e32c0f7e9\", \"2f522840-978a-11ed-893b-4f1e32c0f7e9\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "netsizeString = \"_\".join([str(val) for val in net_sizes])\n",
    "for seed, sessionId in zip(seeds, sessionIds):\n",
    "    details = storingPhase3(sessionId, dataset, seed)\n",
    "    df = df.append(details, ignore_index=True)\n",
    "    filepath = Path(result_dir, dataset, \"{}_{}_.csv\".format(dataset, netsizeString))\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df[df_column_order].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c715b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1061a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44234ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00731175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb947ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prereqResult = run_phase(algorithm, prereqObj)\n",
    "print(\"Prereq result:: \",prereqResult)\n",
    "sessionID = prereqResult['body']['session-id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c824a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 0 \")\n",
    "phase0Obj['session_id'] = sessionID\n",
    "phase0Result = run_phase(algorithm, phase0Obj)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e78411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase :1\")\n",
    "phase1Obj['session_id'] = sessionID\n",
    "phase1Result =run_phase(algorithm, phase1Obj)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f229da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 2 \")\n",
    "phase2Obj['session_id'] = sessionID\n",
    "phase2Result =run_phase(algorithm, phase2Obj)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prereqResult = run_phase(algorithm, prereqObj)\n",
    "# print(\"Prereq result:: \",prereqResult)\n",
    "# sessionID = prereqResult['body']['session-id']\n",
    "# time.sleep(10)\n",
    "# print(\"Phase 0 \")\n",
    "# phase0Obj['session_id'] = sessionID\n",
    "# phase0Result = run_phase(algorithm, phase0Obj)\n",
    "# time.sleep(10)\n",
    "\n",
    "# print(\"Phase :1\")\n",
    "# phase1Obj['session_id'] = sessionID\n",
    "# phase1Result =run_phase(algorithm, phase1Obj)\n",
    "# time.sleep(10)\n",
    "#sessionID=\"04228570-92cb-11ed-8131-97f9b566c791\"\n",
    "\n",
    "# print(\"Phase 2 \")\n",
    "# phase2Obj['session_id'] = sessionID\n",
    "# phase2Result =run_phase(algorithm, phase2Obj)\n",
    "# time.sleep(10)\n",
    "\n",
    "# print(\"Phase 3 \")\n",
    "# phase3Obj['session_id'] = sessionID\n",
    "# phase3Result = run_phase(algorithm, phase3Obj)\n",
    "# time.sleep(10)\n",
    "\n",
    "# print(\"Training Finished !!!\")\n",
    "# phase3Result=fetch_results(algorithm, \"phase3\", sessionID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa5a38",
   "metadata": {},
   "source": [
    "### Fetch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3Result=fetch_results(algorithm, \"phase3\", sessionID)\n",
    "if phase3Result['success']:        \n",
    "    details = {\n",
    "        'dataset': 'Instant_Liking',\n",
    "        'seed': 50, # dummy\n",
    "        'session':sessionID,\n",
    "        'train_rmse': phase3Result['body']['train_error'],\n",
    "        'valid_rmse': phase3Result['body']['validation_error'],\n",
    "        'test_rmse': phase3Result['body']['test_error'],\n",
    "        'best_score':phase3Result['body']['best_score'],\n",
    "        'scores':phase3Result['body']['scores'],\n",
    "        'val_rmses':phase3Result['body']['val_rmses'],\n",
    "        'train_rmses':phase3Result['body']['train_rmses'],\n",
    "        'best_val_lr':phase3Result['body']['best_val_lr'],\n",
    "        'best_fs_num_features':phase3Result['body']['best_fs_num_features'],\n",
    "        'best_model_active_linear_weights':phase3Result['body']['best_model_active_linear_weights'],\n",
    "        'best_model_active_gaussian_weights':phase3Result['body']['best_model_active_gaussian_weights'],\n",
    "        'bound_hitting_gaussian_weights':phase3Result['body']['bound_hitting_gaussian_weights'],\n",
    "        'neuron_centers':phase3Result['body']['neuron_centers'],\n",
    "        'neuron_activity':phase3Result['body']['neuron_activity'],\n",
    "        'radius_map':phase3Result['body']['radius_map'],\n",
    "        'targets':phase3Result['body']['targets'],\n",
    "        'predictions':phase3Result['body']['predictions'],\n",
    "        'total_kernel_time':phase3Result['body']['kernel_time'],\n",
    "        'total_host_time':phase3Result['body']['total_host_time'],\n",
    "        'phase1_kernel_time':phase3Result['body']['phase1_kernel_time'],\n",
    "        'phase1_host_time':phase3Result['body']['phase1_host_time'],\n",
    "        'phase2_kernel_time':phase3Result['body']['phase2_kernel_time'],\n",
    "        'phase2_host_time':phase3Result['body']['phase2_host_time'],\n",
    "        'phase3_kernel_time':phase3Result['body']['phase3_kernel_time'],\n",
    "        'phase3_host_time':phase3Result['body']['phase3_host_time'],\n",
    "        'best_feature_order':phase3Result['body']['best_feature_order'],\n",
    "        'best_feature_list':phase3Result['body']['best_feature_list'],\n",
    "        \n",
    "        }\n",
    "    \n",
    "    df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_feature_order','best_feature_list','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "    df = pd.DataFrame()\n",
    "    netsizeString = \"_\".join([str(val) for val in net_sizes])\n",
    "    \n",
    "    \n",
    "    df = df.append(details, ignore_index=True)\n",
    "    filepath = Path(result_dir, dataset, \"Epochs_{}_seed_{}_{}_.csv\".format(dataset, s, netsizeString))\n",
    "    df[df_column_order].to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a8718",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3Result['body'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_feature_order','best_feature_list','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "\n",
    "dataset =\"Instant_Liking\"\n",
    "df = pd.DataFrame()\n",
    "net_sizes = str(4)\n",
    "seed = str(50)\n",
    "\n",
    "df = df.append(details, ignore_index=True)\n",
    "filepath = Path(result_dir, dataset, \"Epochs_{}_seed_{}_{}_.csv\".format(dataset, seed,net_sizes ))\n",
    "# filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "df[df_column_order].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_feature_order','best_feature_list','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "\n",
    "dataset =\"XOR_noiseless\"\n",
    "df = pd.DataFrame()\n",
    "seeds = 34\n",
    "\n",
    "df = df.append(details, ignore_index=True)\n",
    "filepath = Path(result_dir, dataset, \"Epochs_{}_seed_{}_.csv\".format(dataset, seeds ))\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "# df.to_csv(filepath, index=False)\n",
    "df[df_column_order].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f881150",
   "metadata": {},
   "source": [
    "#### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_column_order = ['dataset','seed','kernel_time','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_feature_order','best_feature_list','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "\n",
    "# dataset, s = \"Facebook\", 50\n",
    "#     # \"Telecom_data\", # -> done\n",
    "#     # \"yearMSD_new\", # -> done\n",
    "#     # \"arrhythmia\", # -> done\n",
    "# #     # \"Big_mart_sales\",\n",
    "#     # \"blogData\", # -> done\n",
    "#     # \"communities\", # -> done\n",
    "#     # \"dengue_features\", # -> done\n",
    "#     # \"ECG0_p02\",\n",
    "#     # \"ENERGY_DATA_COMPLETE\" # -> done\n",
    "# ]\n",
    "# seeds = [50]\n",
    "datasets = [\"House_Price_Adv_Regression\", \"Telecom_data\", \"Insurance\",\"OnlineNewsPopularity\"]\n",
    "df = pd.DataFrame()\n",
    "seeds = [50,100]\n",
    "netsizeString = \"_\".join([str(val) for val in net_sizes])\n",
    "for index, dataset in enumerate(datasets):\n",
    "    for s in tqdm(seeds):\n",
    "        result = process_results(data_directory, dataset,50,df)\n",
    "        df = df.append(result, ignore_index=True)\n",
    "        filepath = Path(result_dir, dataset, \"Epochs_{}_seed_{}_{}_.csv\".format(dataset, s, netsizeString))\n",
    "        df[df_column_order].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_order = ['dataset','seed','session_Id','total_kernel_time','total_host_time','best_feature_order','best_feature_list','train_rmse','valid_rmse','test_rmse','best_score','best_val_lr','best_fs_num_features','best_model_active_linear_weights','best_model_active_gaussian_weights','bound_hitting_gaussian_weights','scores','val_rmses','train_rmses','phase1_host_time','phase2_kernel_time','phase2_host_time','phase3_kernel_time','phase3_host_time','phase1_kernel_time', 'neuron_centers', 'neuron_activity', 'radius_map', 'targets', 'predictions']\n",
    "# datasets = [\n",
    "# #     \"Facebook_data\", # -> done\n",
    "#     \"Features_TestSet\", # -> done\n",
    "#     \"House_Price_Adv_Regression\", # -> done\n",
    "#     \"Instant_Liking\",\n",
    "#     \"Insurance\", # -> done\n",
    "# #     # \"Isolet\" need to redo cuz of some error in RF,\n",
    "#     # \"new_data_trans\", # -> done\n",
    "#     # \"OnlineNewsPopularity\", # -> done\n",
    "#     # \"ParkinsonData\", # -> done\n",
    "#     # \"Sberbank_Russian_Housing_Market\", # -> done\n",
    "#     # \"slice_localization_data\", # -> done\n",
    "#     # \"Telecom_data\", # -> done\n",
    "#     # \"yearMSD_new\", # -> done\n",
    "#     # \"arrhythmia\", # -> done\n",
    "# #     # \"Big_mart_sales\",\n",
    "#     # \"blogData\", # -> done\n",
    "#     # \"communities\", # -> done\n",
    "#     # \"dengue_features\", # -> done\n",
    "#     # \"ECG0_p02\",\n",
    "#     # \"ENERGY_DATA_COMPLETE\" # -> done\n",
    "# ]\n",
    "# seeds = [50]\n",
    "datasets = [\"House_Price_Adv_Regression\", \"Instant_Liking\", \"Insurance\",\"OnlineNewsPopularity\"]\n",
    "df = pd.DataFrame()\n",
    "seeds = [50,100]\n",
    "netsizeString = [str(val) for val in net]\n",
    "for index, dataset in enumerate(datasets):\n",
    "    for s in tqdm(seeds):\n",
    "        result = process_results(data_directory, dataset,50,df)\n",
    "        df = df.append(result, ignore_index=True)\n",
    "        filepath = Path(result_dir, dataset, \"Epochs_{}_seed_{}_.csv\".format(dataset, s ))\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        #df[df_column_order].to_csv(filepath, index=False)\n",
    "        #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46834bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
